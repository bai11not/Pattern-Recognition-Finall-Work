% XGBoost算法实现
function tree = buildXGBoostTree(X, y, gradients, hessians, max_depth, min_samples_split, current_depth, lambda, gamma)
    % 创建一个内部函数来构建树节点
    function node = createTreeNode()
        node = struct('is_leaf', false, 'feature_idx', 0, 'threshold', 0, 'left_child', [], 'right_child', [], 'value', 0, 'gain', 0, 'cover', 0);
    end
    
    % 创建树节点结构体
    tree = createTreeNode();
    tree.is_leaf = true;
    % 参数默认值设置
    if nargin < 9
        gamma = 0.1;  % 树分裂的正则化参数
    end
    if nargin < 8
        lambda = 1.0;  % L2正则化参数
    end
    if nargin < 7
        current_depth = 1;
    end
    if nargin < 6
        min_samples_split = 2;
    end
    if nargin < 5
        max_depth = 6;
    end
    
    % 创建叶节点的条件
    if current_depth >= max_depth || size(X, 1) < min_samples_split
        tree = createTreeNode();
        tree.is_leaf = true;
        
        % 计算叶节点的值
        sum_g = sum(gradients);
        sum_h = sum(hessians);
        
        if sum_h + lambda > 0
            tree.value = -sum_g / (sum_h + lambda);  % 带L2正则化的最优值
        else
            tree.value = 0;
        end
        
        tree.cover = sum_h + lambda;
        return;
    end
    
    % 计算当前节点的总梯度和Hessian
    sum_g_total = sum(gradients);
    sum_h_total = sum(hessians);
    
    % 寻找最佳分裂点
    best_gain = -inf;
    best_feature = 0;
    best_threshold = 0;
    best_left_mask = [];
    best_right_mask = [];
    
    % 遍历所有特征
    for i = 1:size(X, 2)
        % 对特征值进行排序以找到可能的分裂点
        [sorted_vals, idx] = sort(X(:, i));
        sorted_gradients = gradients(idx);
        sorted_hessians = hessians(idx);
        
        % 计算累积梯度和Hessian
        cum_grad_left = cumsum(sorted_gradients);
        cum_hess_left = cumsum(sorted_hessians);
        cum_grad_right = sum_g_total - cum_grad_left;
        cum_hess_right = sum_h_total - cum_hess_left;
        
        % 遍历所有可能的分裂点
        for j = 1:size(X, 1)-1
            % 跳过重复值
            if j < size(X, 1)-1 && sorted_vals(j) == sorted_vals(j+1)
                continue;
            end
            
            % 确保左右子树都有足够的样本
            if j < min_samples_split || (size(X, 1) - j) < min_samples_split
                continue;
            end
            
            % 计算分裂增益
            left_gain = cum_grad_left(j)^2 / (cum_hess_left(j) + lambda);
            right_gain = cum_grad_right(j)^2 / (cum_hess_right(j) + lambda);
            current_gain = cum_grad_left(j)^2 / (cum_hess_left(j) + lambda);
            
            % 计算最终增益
            gain = 0.5 * (left_gain + right_gain - sum_g_total^2 / (sum_h_total + lambda)) - gamma;
            
            if gain > best_gain
                best_gain = gain;
                best_feature = i;
                best_threshold = sorted_vals(j);
                best_left_mask = idx(1:j);
                best_right_mask = idx(j+1:end);
            end
        end
    end
    
    % 如果增益不足以抵消gamma，停止分裂
    if best_gain <= 0
        tree = createTreeNode();
        tree.is_leaf = true;
        
        if sum_h_total + lambda > 0
            tree.value = -sum_g_total / (sum_h_total + lambda);
        else
            tree.value = 0;
        end
        
        tree.cover = sum_h_total + lambda;
        return;
    end
    
    % 创建内部节点并递归构建子树
    tree = createTreeNode();
    tree.is_leaf = false;
    tree.feature_idx = best_feature;
    tree.threshold = best_threshold;
    tree.gain = best_gain;
    tree.cover = sum_h_total + lambda;
    
    % 确保左子树和右子树都有样本
    if isempty(best_left_mask) || isempty(best_right_mask)
        tree.is_leaf = true;
        if sum_h_total + lambda > 0
            tree.value = -sum_g_total / (sum_h_total + lambda);
        else
            tree.value = 0;
        end
        return;
    end
    
    % 递归构建左右子树
    tree.left_child = buildXGBoostTree(X(best_left_mask, :), y(best_left_mask), gradients(best_left_mask), hessians(best_left_mask), max_depth, min_samples_split, current_depth + 1, lambda, gamma);
    tree.right_child = buildXGBoostTree(X(best_right_mask, :), y(best_right_mask), gradients(best_right_mask), hessians(best_right_mask), max_depth, min_samples_split, current_depth + 1, lambda, gamma);
end

//这段代码实现了XGBoost算法中的树构建函数，主要用于梯度提升树模型中的单个决策树构建。以下是其核心功能和技术要点：
// 1. 核心功能
//    1.1 递归构建决策树：通过递归方式构建完整的决策树结构
//    1.2 最优分裂点选择：基于梯度和Hessian信息寻找最佳特征和阈值进行分裂
//    1.3 叶节点值计算：根据带正则化的公式计算叶节点的最优预测值
//    1.4 树生长控制：通过深度限制和样本数阈值控制树的生长

// 2. 技术架构
//    2.1 树结构：基于结构体的二叉决策树
//    2.2 节点类型：支持内部节点（分裂节点）和叶节点
//    2.3 分裂策略：基于梯度统计的增益最大化分裂
//    2.4 正则化机制：集成L2正则化和gamma剪枝

// 3. 关键技术实现
//    3.1 节点结构体设计
//       3.1.1 使用struct定义树节点，包含分裂信息和子节点引用
//       3.1.2 存储节点增益、覆盖度等统计信息
//    3.2 分裂增益计算
//       3.2.1 基于梯度和Hessian的增益计算公式
//       3.2.2 考虑gamma正则化的分裂条件
//    3.3 叶节点最优值计算
//       3.3.1 带L2正则化的叶节点值求解
//       3.3.2 处理数值稳定性（分母检查）
//    3.4 树生长控制
//       3.4.1 最大深度限制
//       3.4.2 最小样本分裂阈值
//       3.4.3 增益阈值剪枝

// 4. 构建流程
//    4.1 初始化树节点和默认参数
//    4.2 检查是否满足叶节点条件（深度限制或样本数限制）
//    4.3 遍历所有特征寻找最佳分裂点
//    4.4 对每个特征，排序并计算累积梯度和Hessian
//    4.5 计算每个可能分裂点的增益
//    4.6 根据最佳增益决定是否分裂
//    4.7 递归构建左右子树

// 具体分析：
// 1. 函数声明和节点结构定义部分（168-178行）
//    1.1 代码作用：
//       这部分定义了一个名为`buildXGBoostTree`的函数，实现了XGBoost算法中的决策树构建
//       函数返回构建好的树结构，采用递归方式实现
//       同时定义了一个内部函数`createTreeNode`用于创建树节点
//    1.2 参数说明：
//       `X`: 特征矩阵，每行为一个样本的特征向量
//       `y`: 标签向量
//       `gradients`: 梯度向量，用于计算分裂增益
//       `hessians`: Hessian向量，用于计算分裂增益
//       `max_depth`: 树的最大深度，控制树的复杂度
//       `min_samples_split`: 分裂所需的最小样本数
//       `current_depth`: 当前节点的深度
//       `lambda`: L2正则化参数
//       `gamma`: 树分裂的正则化参数

// 2. 树节点创建和参数默认值设置部分（179-193行）
//    2.1 代码作用：
//       这部分代码负责创建树节点并设置函数参数的默认值
//       确保函数调用时即使缺少某些参数也能正常工作
//    2.2 具体实现：
//       2.2.1 树节点初始化：
//          `tree = createTreeNode()`: 创建一个新的树节点
//          `tree.is_leaf = true`: 初始时假设节点为叶节点
//       2.2.2 参数默认值设置：
//          使用`if nargin < ...`结构检查输入参数数量并设置默认值
//          `gamma = 0.1`: 树分裂的正则化参数默认值
//          `lambda = 1.0`: L2正则化参数默认值
//          `current_depth = 1`: 当前深度默认从1开始
//          `min_samples_split = 2`: 最小分裂样本数默认值
//          `max_depth = 6`: 最大深度默认值

// 3. 叶节点创建条件检查部分（194-211行）
//    3.1 代码作用：
//       这部分代码检查当前节点是否应该成为叶节点
//       如果满足条件，则计算叶节点的值并返回
//    3.2 具体实现：
//       3.2.1 叶节点条件判断：
//          `if current_depth >= max_depth || size(X, 1) < min_samples_split`: 
//          当达到最大深度或样本数不足时，创建叶节点
//       3.2.2 叶节点值计算：
//          计算总梯度和总Hessian: `sum_g = sum(gradients)`, `sum_h = sum(hessians)`
//          使用带L2正则化的公式计算叶节点值：`tree.value = -sum_g / (sum_h + lambda)`
//          添加分母检查避免除零错误：`if sum_h + lambda > 0`
//       3.2.3 覆盖度计算：
//          `tree.cover = sum_h + lambda`: 计算节点的覆盖度，用于后续统计

// 4. 最佳分裂点搜索部分（212-263行）
//    4.1 代码作用：
//       这部分代码遍历所有特征和可能的分裂点，寻找能够产生最大增益的分裂
//       是XGBoost树构建的核心部分
//    4.2 具体实现：
//       4.2.1 节点统计信息计算：
//          计算当前节点的总梯度和总Hessian: `sum_g_total = sum(gradients)`, `sum_h_total = sum(hessians)`
//       4.2.2 初始化最佳分裂变量：
//          设置初始最佳增益为负无穷: `best_gain = -inf`
//          初始化最佳特征、阈值和左右子树掩码
//       4.2.3 特征遍历：
//          `for i = 1:size(X, 2)`: 遍历所有特征
//       4.2.4 特征值排序：
//          对当前特征的所有值进行排序: `[sorted_vals, idx] = sort(X(:, i))`
//          相应地重新排列梯度和Hessian向量
//       4.2.5 累积统计计算：
//          计算累积梯度和累积Hessian: `cum_grad_left = cumsum(sorted_gradients)`, `cum_hess_left = cumsum(sorted_hessians)`
//          计算右侧累积值: `cum_grad_right = sum_g_total - cum_grad_left`, `cum_hess_right = sum_h_total - cum_hess_left`
//       4.2.6 分裂点遍历：
//          `for j = 1:size(X, 1)-1`: 遍历所有可能的分裂点
//          跳过重复值和不满足最小样本要求的分裂点
//       4.2.7 增益计算：
//          计算左子树和右子树的增益: `left_gain = cum_grad_left(j)^2 / (cum_hess_left(j) + lambda)`, `right_gain = cum_grad_right(j)^2 / (cum_hess_right(j) + lambda)`
//          计算考虑gamma的总增益: `gain = 0.5 * (left_gain + right_gain - sum_g_total^2 / (sum_h_total + lambda)) - gamma`
//          更新最佳分裂信息

// 5. 分裂决策和子树构建部分（264-305行）
//    5.1 代码作用：
//       这部分代码基于找到的最佳分裂点决定是否进行分裂
//       如果分裂增益足够大，则创建内部节点并递归构建子树
//       否则，将当前节点设为叶节点
//    5.2 具体实现：
//       5.2.1 分裂增益判断：
//          `if best_gain <= 0`: 如果最佳增益不大于0，则不分裂，创建叶节点
//       5.2.2 内部节点创建：
//          创建内部节点并设置分裂信息: `tree.is_leaf = false`, `tree.feature_idx = best_feature`, `tree.threshold = best_threshold`
//          存储增益和覆盖度信息
//       5.2.3 样本分配检查：
//          确保左右子树都有样本: `if isempty(best_left_mask) || isempty(best_right_mask)`
//          如果任一子树没有样本，则将当前节点设为叶节点
//       5.2.4 递归构建子树：
//          递归调用`buildXGBoostTree`构建左子树和右子树
//          传递相应的子数据集和参数，深度加1

// 6. XGBoost树构建特点说明：
//    6.1 基于梯度的分裂选择：
//       与传统决策树不同，XGBoost使用梯度和Hessian信息进行分裂选择
//       这种方法更适合集成学习框架，能够更好地优化损失函数
//    6.2 正则化机制：
//       集成了L2正则化和gamma剪枝，有效防止过拟合
//       叶节点值计算和分裂增益计算都考虑了正则化因素
//    6.3 树结构灵活性：
//       通过参数控制树的生长过程，支持不同的数据规模和复杂度
//       递归实现使树的构建更加灵活和高效
//    6.4 数值稳定性考虑：
//       包含分母检查等数值稳定性措施，避免计算错误
