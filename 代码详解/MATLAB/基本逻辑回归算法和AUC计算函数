% 基本逻辑回归算法实现
function [y_pred, y_scores, model] = logisticRegression(X_train, y_train, X_test, lambda, learning_rate, max_iter)
    % 参数默认值
    if nargin < 3
        error('需要至少提供训练数据、标签和测试数据');
    end
    if nargin < 4 || isempty(lambda)
        lambda = 0; % 默认不使用正则化
    end
    if nargin < 5 || isempty(learning_rate)
        learning_rate = 0.01; % 学习率
    end
    if nargin < 6 || isempty(max_iter)
        max_iter = 500; % 最大迭代次数
    end
    
    % 确保标签是列向量
    y_train = y_train(:);
    
    % 获取特征数和样本数
    n_samples = size(X_train, 1);
    n_features = size(X_train, 2);
    
    % 添加偏置项
    X_train_bias = [ones(n_samples, 1), X_train];
    X_test_bias = [ones(size(X_test, 1), 1), X_test];
    
    % 初始化权重（随机初始化）
    theta = zeros(n_features + 1, 1);
    
    % 简单梯度下降训练
    for iter = 1:max_iter
        % 计算预测概率
        z = X_train_bias * theta;
        h = 1 ./ (1 + exp(-z));
        
        % 计算梯度
        gradient = (X_train_bias' * (h - y_train)) / n_samples;
        
        % 添加L2正则化项
        if lambda > 0
            gradient(2:end) = gradient(2:end) + (lambda / n_samples) * theta(2:end);
        end
        
        % 使用简单梯度下降更新权重
        theta = theta - learning_rate * gradient;

    end
    
    % 构建模型
    model.theta = theta;
    model.lambda = lambda;
    model.learning_rate = learning_rate;
    
    % 预测测试集
    z_test = X_test_bias * theta;
    y_scores = 1 ./ (1 + exp(-z_test));
    
    % 预测类别
    y_pred = y_scores > 0.5;
    
    % 确保输出是列向量
    y_pred = y_pred(:);
    y_scores = y_scores(:);

end

% 计算AUC值的函数
function auc = calculateAUC(actualLabels, predictedScores)
    % 确保输入是列向量
    actualLabels = actualLabels(:);
    predictedScores = predictedScores(:);
    
    % 检查数据维度是否匹配
    if length(actualLabels) ~= length(predictedScores)
        error('标签和预测分数维度不匹配');
    end
    
    % 计算正例和负例的数量
    posCount = sum(actualLabels);
    negCount = length(actualLabels) - posCount;
    
    % 特殊情况处理
    if posCount == 0 || negCount == 0
        auc = 0.5;  % 所有样本都是同一类别
        return;
    end
    
    % 方法1：尝试使用升序排序
    [~, idx] = sort(predictedScores);  % 按预测分数升序排序
    sortedLabels = actualLabels(idx);
    
    % 计算正例的排名和
    posRankSum = 0;
    for i = 1:length(sortedLabels)
        if sortedLabels(i) == 1  % 正例 (恶性)
            posRankSum = posRankSum + i;
        end
    end
    
    % 使用标准AUC计算公式
    auc = (posRankSum - posCount*(posCount + 1)/2) / (posCount * negCount);
    
    % 如果AUC太小，可能是预测分数方向问题，尝试反转
    if auc < 0.5
        auc = 1 - auc;
    end
    
    % 确保AUC在0-1范围内
    auc = max(0, min(1, auc));
end

//这段代码实现了基本的逻辑回归算法和AUC(Area Under the Curve)计算函数，用于二分类任务和模型评估。以下是其核心功能和技术要点：
// 1. 核心功能
//    1.1 逻辑回归模型训练：使用梯度下降法训练逻辑回归分类器
//    1.2 模型预测：对测试数据进行预测，输出预测类别和概率分数
//    1.3 AUC指标计算：评估分类模型的性能指标
//    1.4 正则化支持：支持L2正则化，防止过拟合

// 2. 技术架构
//    2.1 线性分类器框架：基于线性模型构建的概率分类器
//    2.2 梯度下降优化：使用简单梯度下降算法优化参数
//    2.3 Sigmoid映射：将线性输出转换为概率值
//    2.4 性能评估体系：实现AUC计算，用于模型评估

// 3. 关键技术实现
//    3.1 参数管理
//       3.1.1 支持参数默认值设置
//       3.1.2 标签标准化处理（确保列向量格式）
//    3.2 模型训练
//       3.2.1 特征扩展（添加偏置项）
//       3.2.2 权重参数随机初始化
//       3.2.3 梯度下降法优化权重
//    3.3 正则化技术
//       3.3.1 L2正则化实现，可通过lambda参数控制强度
//    3.4 AUC计算
//       3.4.1 基于排序和排名的AUC计算方法
//       3.4.2 自动处理预测方向问题
//       3.4.3 特殊情况处理（单一类别数据）

// 4. 执行流程
//    4.1 逻辑回归流程
//       4.1.1 初始化参数和默认值
//       4.1.2 数据预处理（标签格式标准化）
//       4.1.3 添加特征偏置项
//       4.1.4 初始化模型权重
//       4.1.5 梯度下降训练（迭代更新权重）
//       4.1.6 测试集预测（计算概率和类别）
//    4.2 AUC计算流程
//       4.2.1 验证输入数据格式
//       4.2.2 特殊情况检查和处理
//       4.2.3 按预测分数排序
//       4.2.4 计算正例的排名和
//       4.2.5 应用AUC标准公式
//       4.2.6 预测方向验证和校正

// 具体分析：
// 1. 逻辑回归函数声明和参数定义部分（883-901行）
//    1.1 代码作用：
//       这部分定义了一个名为`logisticRegression`的函数，实现了基本的逻辑回归分类器
//       函数返回三个输出参数：`y_pred`（预测类别）、`y_scores`（预测概率分数）和`model`（训练好的模型）
//       函数接收6个输入参数，分别控制模型训练和优化过程
//    1.2 参数说明：
//       `X_train`: 训练数据集的特征矩阵，每行为一个样本的特征向量
//       `y_train`: 训练数据集的标签向量，值为0或1（二分类问题）
//       `X_test`: 测试数据集的特征矩阵，用于最终预测
//       `lambda`: 正则化参数，控制L2正则化的强度
//       `learning_rate`: 学习率，控制梯度下降的步长
//       `max_iter`: 最大迭代次数，控制训练过程的上限
//    1.3 默认参数设置：
//       如果未提供参数，lambda默认为0（不使用正则化）
//       learning_rate默认为0.01
//       max_iter默认为500

// 2. 数据预处理和参数初始化部分（902-914行）
//    2.1 代码作用：
//       这部分代码负责数据预处理和模型参数初始化
//       确保标签格式正确，并准备特征矩阵用于训练
//    2.2 具体实现：
//       2.2.1 标签格式标准化：
//          `y_train = y_train(:)`: 确保标签是列向量格式
//       2.2.2 训练数据信息获取：
//          `n_samples = size(X_train, 1)`: 获取训练样本数量
//          `n_features = size(X_train, 2)`: 获取特征维度
//       2.2.3 添加偏置项：
//          `X_train_bias = [ones(n_samples, 1), X_train]`: 在训练数据前添加全1列作为偏置项
//          `X_test_bias = [ones(size(X_test, 1), 1), X_test]`: 在测试数据前添加全1列作为偏置项
//          偏置项的添加使模型能够学习截距，提高拟合能力
//       2.2.4 权重初始化：
//          `theta = zeros(n_features + 1, 1)`: 将权重参数初始化为零向量
//          包含n_features+1个元素（对应n_features个特征和1个偏置项）

// 3. 梯度下降训练部分（915-928行）
//    3.1 代码作用：
//       这部分代码实现了逻辑回归的核心训练过程，使用梯度下降法优化权重参数
//       迭代执行前向计算、梯度计算和参数更新
//    3.2 具体实现：
//       3.2.1 迭代控制：
//          `for iter = 1:max_iter`: 执行指定次数的迭代训练
//       3.2.2 前向计算：
//          `z = X_train_bias * theta`: 计算线性输出
//          `h = 1 ./ (1 + exp(-z))`: 应用Sigmoid函数，将线性输出转换为概率
//       3.2.3 梯度计算：
//          `gradient = (X_train_bias' * (h - y_train)) / n_samples`: 计算梯度
//          梯度计算考虑了所有训练样本的预测误差
//          除以样本数量进行归一化，使梯度大小与样本数量无关
//       3.2.4 正则化应用：
//          当lambda > 0时，对除偏置项外的权重添加L2正则化项
//          正则化梯度：`gradient(2:end) = gradient(2:end) + (lambda / n_samples) * theta(2:end)`
//       3.2.5 参数更新：
//          `theta = theta - learning_rate * gradient`: 使用梯度下降更新权重
//          权重更新方向与梯度相反，步长由学习率控制

// 4. 模型构建和预测部分（929-941行）
//    4.1 代码作用：
//       这部分代码负责构建模型对象并对测试数据进行预测
//       生成最终的分类结果和概率分数
//    4.2 具体实现：
//       4.2.1 模型构建：
//          创建model结构体，存储训练好的权重和超参数
//          包括theta（权重）、lambda（正则化参数）和learning_rate（学习率）
//       4.2.2 测试集预测：
//          `z_test = X_test_bias * theta`: 计算测试集的线性输出
//          `y_scores = 1 ./ (1 + exp(-z_test))`: 应用Sigmoid函数，获取预测概率
//          `y_pred = y_scores > 0.5`: 使用0.5作为阈值，将概率转换为类别预测
//       4.2.3 输出格式标准化：
//          确保y_pred和y_scores都是列向量格式

// 5. AUC计算函数实现部分（942-992行）
//    5.1 函数声明和参数定义：
//       函数名为`calculateAUC`，接收两个参数：`actualLabels`（实际标签）和`predictedScores`（预测概率分数）
//       返回计算得到的AUC值
//    5.2 输入验证和预处理：
//       确保输入是列向量格式
//       检查标签和预测分数维度是否匹配
//    5.3 正例和负例计数：
//       `posCount = sum(actualLabels)`: 计算正例数量
//       `negCount = length(actualLabels) - posCount`: 计算负例数量
//    5.4 特殊情况处理：
//       如果只有正例或只有负例，返回AUC=0.5（随机猜测水平）
//    5.5 AUC核心计算：
//       按预测分数升序排序样本
//       计算正例的排名和（posRankSum）
//       应用标准AUC公式：`auc = (posRankSum - posCount*(posCount + 1)/2) / (posCount * negCount)`
//       这个公式基于这样一个事实：AUC等于随机选择的正例得分高于随机选择的负例得分的概率
//    5.6 结果校正和限制：
//       如果计算得到的AUC小于0.5，说明预测方向可能错误，将其反转（auc = 1 - auc）
//       确保AUC值在0到1之间
